
import { genkit, z } from 'genkit';
import { chunkMarkdown } from '@/lib/markdown-chunker';
import { ai } from '@/ai/genkit';

/**
 * Schema for the enriched metadata generated by the LLM.
 * This metadata is crucial for improving the retrieval accuracy of the vector database.
 */
const EnrichedMetadataSchema = z.object({
  summary: z.string().describe("A concise one-sentence summary of the chunk's content."),
  keywords: z.array(z.string()).describe("3-5 key concepts or terms found in this chunk."),
  questions: z.array(z.string()).describe("2 hypothetical questions that this chunk answers."),
});

/**
 * Helper function to process items in batches to avoid hitting API rate limits.
 */
async function runInBatches<T, R>(
  items: T[],
  batchSize: number,
  fn: (item: T, index: number) => Promise<R>
): Promise<R[]> {
  const results: R[] = [];
  for (let i = 0; i < items.length; i += batchSize) {
    const batch = items.slice(i, i + batchSize);
    // Process the current batch in parallel
    const batchResults = await Promise.all(
      batch.map((item, index) => fn(item, i + index))
    );
    results.push(...batchResults);
  }
  return results;
}

/**
 * A Genkit Flow that implements the "Optimized Indexing Pipeline".
 * 
 * This flow takes a raw Markdown document and transforms it into a list of
 * "Enriched Chunks" ready for insertion into a Vector Database.
 * 
 * The pipeline consists of three main steps:
 * 1. **Deterministic Chunking:** Splits the document based on structure (headers).
 * 2. **Semantic Enrichment:** Uses an LLM to generate metadata (Summary, Keywords, Questions).
 * 3. **Embedding Generation:** Converts the enriched text into a vector for semantic search.
 */
export const optimizedIndexingFlow = ai.defineFlow(
  {
    name: 'optimizedIndexingFlow',
    inputSchema: z.object({
      courseId: z.string().describe("The unique ID of the course this document belongs to."),
      documentTitle: z.string().describe("The title of the document (e.g., 'Chapter 1: Introduction')."),
      markdownContent: z.string().describe("The raw Markdown content of the document."),
    }),
    outputSchema: z.object({
      chunksCreated: z.number().describe("The total number of chunks generated."),
      enrichedChunks: z.array(z.object({
        id: z.string(),
        content: z.string(),
        embedding: z.array(z.number()).describe("The vector embedding of the chunk content."),
        metadata: z.object({
          headerPath: z.array(z.string()),
          summary: z.string(),
          keywords: z.array(z.string()),
          questions: z.array(z.string()),
        })
      }))
    }),
  },
  async (input) => {
    console.log(`Starting optimized indexing for: ${input.documentTitle}`);

    // -----------------------------------------------------------------------
    // Step 1: Deterministic Chunking
    // -----------------------------------------------------------------------
    const rawChunks = chunkMarkdown(input.markdownContent);
    console.log(`Generated ${rawChunks.length} raw chunks.`);

    // -----------------------------------------------------------------------
    // Step 2 & 3: Enrichment & Embedding (Batched)
    // -----------------------------------------------------------------------
    // We process chunks in batches of 3 to respect API rate limits.
    const enrichedChunks = await runInBatches(rawChunks, 3, async (chunk, index) => {
      
      const context = `
        Document Title: ${input.documentTitle}
        Section Context: ${chunk.metadata.headerPath.join(' > ')}
        Content:
        ${chunk.content}
      `;

      try {
        // A. Generate Metadata (LLM)
        const { output } = await ai.generate({
          prompt: `
            You are an expert content indexer for a course platform.
            Analyze the following text chunk and generate metadata to improve retrieval.
            
            ${context}
          `,
          output: { schema: EnrichedMetadataSchema },
        });

        if (!output) throw new Error("No output from LLM");

        // B. Generate Embedding (Vector)
        // We embed the content combined with the summary for better semantic density.
        const textToEmbed = `Title: ${input.documentTitle}\nContext: ${chunk.metadata.headerPath.join(' > ')}\nSummary: ${output.summary}\nContent: ${chunk.content}`;
        
        const embeddingResult = await ai.embed({
          embedder: 'googleai/text-embedding-004', 
          content: textToEmbed
        });

        return {
          id: `${input.courseId}-${index}`,
          content: chunk.content,
          embedding: embeddingResult[0].embedding, // Extract the vector from the first result
          metadata: {
            headerPath: chunk.metadata.headerPath,
            ...output
          }
        };
      } catch (error) {
        console.error(`Failed to process chunk ${index}:`, error);
        // Fallback: Return chunk without enrichment/embedding if it fails
        return {
          id: `${input.courseId}-${index}`,
          content: chunk.content,
          embedding: [], // Empty embedding on failure
          metadata: {
            headerPath: chunk.metadata.headerPath,
            summary: "Processing failed",
            keywords: [],
            questions: []
          }
        };
      }
    });

    console.log(`Successfully processed ${enrichedChunks.length} chunks.`);

    return {
      chunksCreated: enrichedChunks.length,
      enrichedChunks
    };
  }
);
