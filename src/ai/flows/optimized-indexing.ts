
import { genkit, z } from 'genkit';
import { chunkMarkdown } from '@/lib/markdown-chunker';
import { ai } from '@/ai/genkit';

/**
 * Schema for the enriched metadata generated by the LLM.
 * This metadata is crucial for improving the retrieval accuracy of the vector database.
 */
const EnrichedMetadataSchema = z.object({
  summary: z.string().describe("A concise one-sentence summary of the chunk's content."),
  keywords: z.array(z.string()).describe("3-5 key concepts or terms found in this chunk."),
  questions: z.array(z.string()).describe("2 hypothetical questions that this chunk answers."),
});

/**
 * A Genkit Flow that implements the "Optimized Indexing Pipeline".
 * 
 * This flow takes a raw Markdown document and transforms it into a list of
 * "Enriched Chunks" ready for insertion into a Vector Database.
 * 
 * The pipeline consists of two main steps:
 * 1. **Deterministic Chunking:** Splits the document based on structure (headers) using `chunkMarkdown`.
 * 2. **Semantic Enrichment:** Uses an LLM (Gemini) to generate summaries, keywords, and questions for each chunk.
 */
export const optimizedIndexingFlow = ai.defineFlow(
  {
    name: 'optimizedIndexingFlow',
    inputSchema: z.object({
      courseId: z.string().describe("The unique ID of the course this document belongs to."),
      documentTitle: z.string().describe("The title of the document (e.g., 'Chapter 1: Introduction')."),
      markdownContent: z.string().describe("The raw Markdown content of the document."),
    }),
    outputSchema: z.object({
      chunksCreated: z.number().describe("The total number of chunks generated."),
      enrichedChunks: z.array(z.object({
        id: z.string(),
        content: z.string(),
        metadata: z.object({
          headerPath: z.array(z.string()),
          summary: z.string(),
          keywords: z.array(z.string()),
          questions: z.array(z.string()),
        })
      }))
    }),
  },
  async (input) => {
    console.log(`Starting optimized indexing for: ${input.documentTitle}`);

    // -----------------------------------------------------------------------
    // Step 1: Deterministic Chunking
    // -----------------------------------------------------------------------
    // We use our custom utility to split the markdown while preserving header context.
    // This ensures that even small chunks know they belong to "Chapter 1 > Section 2".
    const rawChunks = chunkMarkdown(input.markdownContent);
    console.log(`Generated ${rawChunks.length} raw chunks.`);

    // -----------------------------------------------------------------------
    // Step 2: Parallel LLM Enrichment
    // -----------------------------------------------------------------------
    // We iterate over each raw chunk and ask the LLM to analyze it.
    // Promise.all allows us to process all chunks concurrently for speed.
    // Note: In a high-volume production environment, consider using a concurrency limiter (like p-limit).
    const enrichedChunksPromises = rawChunks.map(async (chunk, index) => {
      
      // Construct a prompt context that gives the LLM the full picture.
      // We include the "Section Context" (header path) so the LLM understands where this text fits.
      const context = `
        Document Title: ${input.documentTitle}
        Section Context: ${chunk.metadata.headerPath.join(' > ')}
        Content:
        ${chunk.content}
      `;

      try {
        // Call the AI model to generate the metadata defined in EnrichedMetadataSchema.
        const { output } = await ai.generate({
          prompt: `
            You are an expert content indexer for a course platform.
            Analyze the following text chunk and generate metadata to improve retrieval.
            
            ${context}
          `,
          output: { schema: EnrichedMetadataSchema },
        });

        if (!output) throw new Error("No output from LLM");

        // Return the fully enriched chunk object.
        return {
          id: `${input.courseId}-${index}`, // Unique ID for the chunk
          content: chunk.content,
          metadata: {
            headerPath: chunk.metadata.headerPath,
            ...output // Spread the AI-generated summary, keywords, and questions
          }
        };
      } catch (error) {
        console.error(`Failed to enrich chunk ${index}:`, error);
        // Fallback Strategy: 
        // If the LLM fails (e.g., rate limit, network error), we still return the chunk
        // but with empty metadata so we don't lose the data entirely.
        return {
          id: `${input.courseId}-${index}`,
          content: chunk.content,
          metadata: {
            headerPath: chunk.metadata.headerPath,
            summary: "Processing failed",
            keywords: [],
            questions: []
          }
        };
      }
    });

    // Wait for all chunks to be processed.
    const enrichedChunks = await Promise.all(enrichedChunksPromises);

    console.log(`Successfully enriched ${enrichedChunks.length} chunks.`);

    return {
      chunksCreated: enrichedChunks.length,
      enrichedChunks
    };
  }
);
